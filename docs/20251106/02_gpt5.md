Love this! Your layout is already thoughtful and OTP-friendly. Below are “tightening” recommendations that keep the current surface area but make the internals more generic, resilient, and idiomatic for OTP—so it’s easier to integrate into real apps (LiveView, Broadway, oban workers, etc.) without surprising behavior.

I’ll group changes by theme and include small, surgical code tweaks you can drop in.

# 1) OTP process model: make the connection a proper state machine

**Why:** Connection lifecycles (spawn/handshake/ready/retry/closing) are easier and safer in a `:gen_statem` than a plain `GenServer`.

**What to change**

* Move `lib/mcp_client/connection/state.ex` logic into a `gen_statem` (`McpClient.Connection`), with explicit states: `:starting → :initializing → :ready → :backoff → :closing`.
* Keep `McpClient.Client` as a thin façade that delegates to the statem PID.

```elixir
# lib/mcp_client/connection.ex
defmodule McpClient.Connection do
  @behaviour :gen_statem
  alias McpClient.{Transport, Types}

  @type data :: %{
    transport: pid(),
    req_mgr: pid(),
    caps: Types.ServerCapabilities.t() | nil,
    init_timer: reference() | nil,
    backoff: non_neg_integer()
  }

  @impl true
  def callback_mode, do: :handle_event_function

  def start_link(opts),
    do: :gen_statem.start_link(__MODULE__, opts, [])

  @impl true
  def init(opts) do
    Process.flag(:trap_exit, true)
    data = %{transport: nil, req_mgr: nil, caps: nil, init_timer: nil, backoff: 1000}
    {:ok, :starting, data, {:next_event, :internal, {:spawn_transport, opts}}}
  end

  # … handle :starting, :initializing, :ready, :backoff, :closing
end
```

**Benefits**

* Clear transitions; fewer “impossible” states.
* Backoff/reattempts are centralized and easier to reason about.
* Graceful shutdown (trap exits) lands in `:closing` consistently.

# 2) Transport abstraction: lock down a minimal, synchronous contract

**Why:** Your transport is a boundary. Keep it small and predictable; push framing/ordering responsibilities *into* the transport.

**What to change**

* Define a **strict** behaviour with 4 callbacks and no optional ones:

  * `init/1` (spawn Port / open HTTP/SSE / stdio)
  * `send/2` (binary)
  * `set_active/2` (`:once | false`) to control backpressure
  * `close/1`
* Transports must emit **already-framed** JSON-RPC payloads (one binary per message). The connection process should not parse half frames.

```elixir
# lib/mcp_client/transport/behaviour.ex
@callback init(keyword()) :: {:ok, pid()} | {:error, term()}
@callback send(pid(), iodata()) :: :ok | {:error, term()}
@callback set_active(pid(), :once | false) :: :ok | {:error, term()}
@callback close(pid()) :: :ok
```

**Notes**

* For stdio, use `Port` with `{packet, 4}` or NDJSON with explicit line framing—pick one and be consistent. JSON-RPC over NDJSON is common; if so, do the line splitting inside the transport and only deliver complete JSON strings upstream.
* For SSE, the transport should reconstruct event data chunks and push complete JSON strings.

# 3) Backpressure & mailbox hygiene

**Why:** Long-running tools and high-volume notifications will flood mailboxes.

**What to change**

* In transport processes, use **active once** semantics and only re-enable after the connection consumes a message:

```elixir
# connection.ex (on message)
handle_event(:info, {:transport, :message, json}, :ready, data) ->
  # process JSON
  _ = Transport.set_active(data.transport, :once)
  {:keep_state, data}
```

* Batch emit telemetry and logging from a dedicated lightweight process to keep the hot path clean.

# 4) Requests: references over blocking calls (without adding new features)

You already expose synchronous functions. Keep them, but **internally** route everything through a request manager that:

* Assigns **monotonic** ids with `System.unique_integer([:positive, :monotonic])`.
* Stores waiters in ETS keyed by id with `{id, from, timer_ref}`.
* Correlates responses and replies directly to the caller to avoid extra mailbox hops.

```elixir
id = System.unique_integer([:positive, :monotonic])
true = :ets.insert(:mcp_requests, {id, from, start_timer(id, timeout)})
Transport.send(t, encode_jsonrpc(id, method, params))
```

**Timeouts**

* Use a single `:timer` per request entry (not a per-process timer wheel).
* On timeout, remove from ETS and reply `{:error, :timeout}`.
* Consider allowing per-call override (you already do) but enforce a **hard ceiling** from config to prevent unbounded waits.

# 5) Cancellation: make it reliable (you already claim support)

You don’t need to add a public API; just guarantee correctness under the hood:

* If a GenServer call is cancelled (caller terminates) or times out, send JSON-RPC cancel (`"$/cancelRequest"`) and **ignore** any late response for that id.
* Keep a “cancelled id” set for a short horizon to drop stragglers.

# 6) Notification delivery: decouple from client process

**Why:** User callbacks must never stall the connection.

**What to change**

* Keep `on_notification/2` but deliver via a `Task.Supervisor` (supervised child) per notification, or a local `PartitionedDispatcher` using `Registry` and `DynamicSupervisor`.
* If callbacks raise, swallow in the task and emit telemetry—not an exit that takes down the connection.

```elixir
# client.ex
def handle_cast({:register_notification_handler, fun}, state) do
  {:noreply, %{state | handlers: [fun | state.handlers]}}
end

defp dispatch_notification(state, notif) do
  for fun <- state.handlers do
    Task.Supervisor.start_child(state.notif_sup, fn -> fun.(notif) end)
  end
  :ok
end
```

# 7) Public API polish: naming, spec, and consistent options

* **Timeout option key**: always `:timeout` at the top level of opts; don’t use different defaults per call unless necessary. Document the defaults in moduledocs and enforce with `NimbleOptions` so invalid options fail early.
* **Return types**: expose an opaque `t:client_ref/0` type and avoid exposing internal PIDs in docs.

```elixir
@opaque client_ref :: pid() | atom()
@type uri :: String.t()
```

* **Synchronous calls**: keep them, but ensure they never block the connection loop: only the **caller** is blocked by a direct reply from the request tracker.

# 8) Errors: one exception struct, fully Dialyzer-annotated

* Collapse error types under `McpClient.Error` with fields `:code, :message, :data, :kind` (`:jsonrpc | :transport | :protocol | :state`).
* Implement `Exception` so `raise McpClient.Error` renders helpful text.
* Prefer returning `{:error, %McpClient.Error{}}` everywhere (you already do) and remove stray atoms like `:not_initialized`—wrap them in the struct for consistency (you can keep the old atoms for now but mark deprecated).

```elixir
defmodule McpClient.Error do
  defexception [:code, :message, :data, :kind]
  @type t :: %__MODULE__{code: integer() | nil, message: String.t(), data: term(), kind: atom()}
  def message(%__MODULE__{message: m, kind: k, code: c}), do: "[#{k}#{if c, do: "/#{c}", else: ""}] #{m}"
end
```

# 9) JSON handling: predictable framing & tolerant decoder

* Pin `Jason` and configure `:escape`/`:bytes_per_redution` as needed for large payloads.
* Guard all decode errors and map to `%Error{kind: :protocol}` with the offending payload size (but never log the entire body at `:info`).

# 10) Telemetry: stable event schema

Your event names are good; define a **single event map schema** so downstream users know what to expect:

* `[:mcp_client, :request, :start | :stop | :exception]`

  * `meta`: `%{client: pid(), method: String.t(), id: integer()}`
  * `measurements`: `%{system_time: integer()} | %{duration: native_time()}`
* `[:mcp_client, :connection, :transition]`

  * `meta`: `%{from: atom(), to: atom(), reason: term() | nil}`
* `[:mcp_client, :notification, :received]`

  * `meta`: `%{method: String.t()}`

Document these **exact keys** in `middleware/telemetry.ex` and keep them stable.

# 11) Supervision: predictable child layout

Make the supervision tree explicit and small:

```
McpClient.Application
└─ McpClient.Connection.Supervisor (one_for_one)
   ├─ Registry (keys: :unique)         # McpClient.Connection.Registry
   └─ DynamicSupervisor                # McpClient.Connection.Dynamic
       └─ McpClient.Connection (gen_statem)
          ├─ McpClient.Transport (Port/Task/Finch)
          ├─ RequestManager (GenServer+ETS)
          └─ Task.Supervisor (notifications/progress)
```

* Start the `Registry` once at app init.
* Name each connection child deterministically (or via registry) so `McpClient.start_link(name: ...)` is reliable.
* Consider `restart: :transient` for transports (they can crash legitimately) while the connection performs backoff.

# 12) Testability & CI polish (no new features)

* Use `Mox` instead of bespoke `MockServer` for transport and protocol surfaces. Keep your `mock_server.ex` for end-to-end but mock at boundaries to assert framing and id correlation.

* Add property tests with `StreamData` for:

  * request id uniqueness
  * response/id correlation under reordering
  * cancellation idempotency

* Add `ExUnit.CaseTemplate` in `test/support` for starting a supervised connection per test with isolated ETS tables.

* Add `dialyzer` to `mix.exs` with a clean PLT and `@opaque` client ref.

# 13) Docs & ergonomics (without more features)

* Keep the README API examples, but add a **“OTP Integration Patterns”** section with:

  * Supervised singleton client
  * Dynamic per-session client (e.g., LiveView mount → start, terminate → stop, using `:temporary` restart)
  * Background worker client (Oban/Broadway) recommending `:transient` restart and `await_initialized/2` with a longer timeout

* Provide a tiny `start_spec/1` helper to encourage supervision usage over manual `start_link/1`:

```elixir
@doc "Return a child spec to run the client under a supervisor."
@spec child_spec(keyword()) :: Supervisor.child_spec()
def child_spec(opts), do: %{
  id: {__MODULE__, Keyword.get(opts, :name, make_ref())},
  start: {__MODULE__, :start_link, [opts]},
  restart: :permanent,
  type: :worker,
  shutdown: 15_000
}
```

# 14) Small API nits

* `await_initialized/2`: default 10_000 in docs but 5_000 in code. Make both 10_000 (or 30_000 for slow servers) and document clearly.
* `on_notification/2` and `on_progress/2` currently accept only *one* handler function by sending a cast. You’re storing multiple—good—just make order explicit (LIFO) and document that handlers run concurrently and must be pure/non-blocking.
* `state/1` returning atoms is fine; consider returning `{:ok, atom()}` to keep return shapes consistent (`{:ok, _} | {:error, _}`) without adding a new function.

# 15) Minimal code edits you can apply now

**a) Consistent option validation with NimbleOptions**

```elixir
# mix.exs deps
{:nimble_options, "~> 1.0", runtime: false}

# lib/mcp_client/options.ex
defmodule McpClient.Options do
  @schema [
    transport: [type: {:in, [:stdio, :sse, :http_sse]}, required: true],
    timeout: [type: :non_neg_integer, default: 30_000],
    initialize_timeout: [type: :non_neg_integer, default: 10_000],
    client_info: [type: :map, default: %{}],
    name: [type: {:or, [:atom, :pid, :nil]}, default: nil],
    command: [type: :string],
    args: [type: {:list, :string}, default: []],
    env: [type: :map, default: %{}],
    url: [type: :string],
    base_url: [type: :string],
    sse_endpoint: [type: :string],
    message_endpoint: [type: :string],
    headers: [type: {:list, {:tuple, [:string, :string]}}, default: []],
    middleware: [type: {:list, :any}, default: []]
  ]
  def validate!(opts), do: NimbleOptions.validate!(opts, @schema)
end
```

Use it in `start_link/1` before starting processes.

**b) Monotonic ids & ETS table**

```elixir
# request/manager.ex (init)
:ets.new(:mcp_requests, [:set, :protected, :named_table, read_concurrency: true])

def next_id(), do: System.unique_integer([:positive, :monotonic])
```

**c) Robust cancellation**

```elixir
def handle_call({:call_tool, name, args, opts}, from, state) do
  id = next_id()
  tref = Process.send_after(self(), {:timeout, id}, opts[:timeout] || state.timeout)
  :ets.insert(:mcp_requests, {id, from, tref})
  send_json(state, id, "tools/call", %{name: name, arguments: args})
  {:noreply, state}
end

def handle_info({:timeout, id}, state) do
  case :ets.take(:mcp_requests, id) do
    [{^id, from, _tref}] ->
      send_json(state, nil, :"$/cancelRequest", %{id: id})
      GenServer.reply(from, {:error, %McpClient.Error{kind: :timeout, message: "request timed out"}})
    _ -> :ok
  end
  {:noreply, state}
end
```

**d) Transport backpressure**
After each processed frame, call `Transport.set_active(transport, :once)`; on bursts, you will only receive again once you’re ready.

---

## How this helps real OTP integrations

* **LiveView or Telemetry dashboards:** Notifications are dispatched via supervised tasks—no freezes from a slow render or heavy JSON decode in a UI process.
* **Broadway/Oban workers:** Synchronous calls won’t overflow the connection because the transport is active-once and the request manager isolates timeouts/cancellation.
* **Long-running tools:** Late responses don’t leak memory; cancelled ids are dropped without touching user callbacks.

If you’d like, I can sketch the `gen_statem` transitions and a tiny `stdio` transport using `Port` + NDJSON framing, or tune the `Telemetry` event schema into a ready-to-copy docstring.
