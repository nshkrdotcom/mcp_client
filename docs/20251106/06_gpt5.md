This is fantastic feedback. I folded each refinement into the design so it’s explicit, mechanically tight, and easy to implement.

---

# 1) State machine refinement: add `:re_syncing` (transient)

## New states

* `:re_syncing` — **transient** cleanup/renegotiation gate between `:ready` and `:initializing`.

## Updated transitions (delta)

```
:ready
  proto(:"notifications/cancelled")    ⇒ → :re_syncing

:re_syncing (transient)
  entry actions:
    - clear requests; tombstone all in-flight ids
    - bump session_id (see §2)
    - emit telemetry [:connection, :transition] with reason :server_cancelled
  ctl({:user_call, ...})               ⇒ reply {:error, :re_syncing}, stay
  int({:internal, :cleanup_done})      ⇒ → :initializing
  io({:transport_down, reason})        ⇒ → :backoff({:transport_down, reason})
```

**Why this helps**

* Callers get a precise, short-lived error (`:re_syncing`) during a server-wide reset.
* No race: requests are canceled before re-init begins; new calls are rejected consistently until we re-enter `:initializing`.

**Implementation nits**

* Use `postpone` sparingly; prefer immediate replies in `:re_syncing`.
* Mark `:re_syncing` with a **very short** guard timer (e.g., 10–50ms) to sequence cleanup → init deterministically:

  * on enter: `{:keep_state_and_data, [{:next_event, :internal, :cleanup_done}]}`
  * no sleeps; cleanup is O(#inflight).

---

# 2) Tombstones + session_id (anti-stale guarantee)

Add a monotonic `session_id` that changes every time we transition **into** `:ready` after a successful init. It’s recorded per request and validated on responses.

## Data

```elixir
data = %{
  session_id: non_neg_integer(),
  requests: %{id => %{from: from, deadline: t_mono, corr_id: bin, session_id: sid}},
  tombstones: :ets | map
}
```

## Rules

* On `:initializing → :ready`: `session_id = System.unique_integer([:positive, :monotonic])`
* On request: write `session_id` into the row.
* On response: deliver **only if** `row.session_id == data.session_id`; else drop (stale frame).
* On `:re_syncing` entry: **bump `session_id` immediately**, tombstones all current rows, then proceed to init.

**Benefit**

* Even if a partition outlasts the tombstone TTL, late responses cannot be misdelivered—they’re rejected by session mismatch.

---

# 3) Supervision/linking: explicit signal choreography

**Transport dies**

1. `Transport` exits → Supervisor (rest_for_one) **restarts Transport**.
2. Because `Connection` is **linked** to Transport, it receives `{'EXIT', transport_pid, reason}`.
3. `Connection` (trap_exit) maps to internal event `io({:transport_down, reason})` and transitions to `:backoff` (or it may also crash due to link—see below).
4. Since `Connection` is **after** Transport in the child list, rest_for_one restarts `Connection` after Transport.
   → New `Connection` binds to the new `Transport` PID and re-initializes.

**Connection dies**

* Link causes Transport to receive exit (default `:normal`/`:shutdown` is not contagious). To ensure clean restart pairing, we **link both ways and monitor**:

  * Transport reacts to `Connection` demise with no special action; Supervisor restarts **Connection only**.
  * At `Connection:init/1`, if it finds a stale Transport PID (race from restarts), it treats it as down and waits for Supervisor to supply the live child (or fails fast). This keeps ownership in the Supervisor.

This removes ambiguity and guarantees both processes converge to a consistent fresh pair.

---

# 4) Backpressure fairness: call it out explicitly

We’ll **document** the trade-off:

> The pipeline prioritizes stability over fairness under mixed large/small frame loads. A single large decode may occupy a decoder slot; if `max_decoders` is reached, `set_active(:once)` is deferred, which can delay subsequent small frames (head-of-line blocking). If strict fairness is required, consider:
>
> * a two-queue decoder (small-frame fast lane) or
> * a weighted concurrency limiter.
>
> These add complexity and overhead; default behavior is tuned for predictable latency and bounded memory.

Config knobs to surface:

```elixir
decode_threshold_bytes: 128_000
max_decoders: 8
decoder_fair_small_bytes: 16_384   # optional future: reserve 1 slot for “small” frames
```

We won’t implement small-frame reservation now; we only **acknowledge** it.

---

# 5) `send_frame/2` busy retry: formalized in `gen_statem`

We implement a single retry using a state timeout—no blocking, no external processes.

```elixir
# happy path
case Transport.send_frame(t, frame) do
  :ok -> set_active_once(); keep_state_and_data()
  {:error, :busy} ->
    actions = [{:state_timeout, 2, {:retry_send, id, frame}}]  # 2ms jittered
    {:keep_state, put_retry(data, id), actions}
  {:error, reason} ->
    reply(from, {:error, %Error{kind: :transport, message: "send failed", data: reason}})
    {:keep_state, del_request(data, id)}
end

# timeout handler
handle_event(:state_timeout, {:retry_send, id, frame}, :ready, data) ->
  case Transport.send_frame(data.transport, frame) do
    :ok -> set_active_once(); {:keep_state, clear_retry(data, id)}
    {:error, :busy} ->
      reply_timeout_backpressure(id, data) # {:error, :backpressure}
      {:keep_state, del_request(data, id)}
    {:error, reason} ->
      reply_send_failed(id, reason, data)
      {:keep_state, del_request(data, id)}
  end
```

This confines busy handling to the state machine and makes the outcome deterministic.

---

# 6) Caller outcomes during transient phases

Add explicit result codes for clarity:

* `{:error, :re_syncing}` – server-wide reset cleanup in progress
* `{:error, :unavailable}` – `:backoff` or `:initializing`
* `{:error, :backpressure}` – transport send busy after retry
* `{:error, :cancelled_by_server}` – server canceled already-accepted request

These map cleanly to observability and user logic.

---

# 7) Telemetry: include transient states and reasons

Extend transition event to surface these distinct modes:

```
[:mcp_client, :connection, :transition]
meta: %{
  from: :ready | :initializing | :backoff | :re_syncing | :closing,
  to:   same,
  reason: :server_cancelled | {:transport_down, reason} | :init_ok | :init_timeout | :stop
}
```

Now `:re_syncing` is fully visible in traces/dashboards.

---

# 8) Minimal code deltas you can drop in

### a) Data additions

```elixir
defstruct session_id: 0,
          requests: %{},     # or ETS
          tombstones: nil,
          in_flight_decoders: 0,
          max_decoders: 8,
          decode_threshold: 128_000
```

### b) Bump session on init-ok

```elixir
when is_init_ok(msg) ->
  sid = System.unique_integer([:positive, :monotonic])
  {:next_state, :ready, %{data | session_id: sid}, telemetry_transition(:init_ok)}
```

### c) Re-sync entry actions

```elixir
def handle_event(:internal, :enter_resync, :ready, data) do
  sid = System.unique_integer([:positive, :monotonic])
  data = tombstone_all_and_clear(data) |> Map.put(:session_id, sid)
  {:next_state, :re_syncing, data, [{:next_event, :internal, :cleanup_done}]}
end

def handle_event(:internal, :cleanup_done, :re_syncing, data) do
  {:next_state, :initializing, data}
end
```

### d) Response delivery gate

```elixir
case lookup_request(id) do
  %{from: from, session_id: sid} = row when sid == data.session_id ->
    delete_request(id)
    reply(from, {:ok, parsed})
  _missing_or_old ->
    drop_frame() # stale or already handled
end
```

---

# 9) Documentation deltas (clarity)

* **State machine** now includes `:re_syncing` with a definition and error returned to callers.
* **Guarantee**: “Exactly one terminal outcome per request” holds even across partitions **longer than tombstone TTL** due to `session_id` gating.
* **Busy handling**: single retry with short timer; deterministic failure as `:backpressure`.
* **Fairness trade-off**: documented explicitly (and why).

---

# 10) Test deltas (property additions)

* **Session safety under long partitions**
  Drive: send N requests, rotate `session_id` via simulated re-inits, then deliver late responses from old sessions.
  Property: zero late deliveries (all dropped), no crashes.

* **Re-syncing rejection window**
  Inject `notifications/cancelled`; fuzz arrivals of user calls around the transition micro-window.
  Property: all calls during the window receive `{:error, :re_syncing}`; none are accepted then cancelled.

* **Busy retry determinism**
  Stub transport to return `:busy` once, then `:ok`; ensure exactly one retry and success.
  Stub twice-busy to ensure `:backpressure` response and cleaned request table.

---

## Closing note

Your original design nailed the big rocks. These refinements make the micro-behaviors airtight:

* `:re_syncing` makes the reset path explicit and observable,
* `session_id` eliminates stale deliveries beyond tombstone horizons,
* formal busy-retry + explicit outcomes sharpen the hot path,
* and we’ve documented the fairness trade-off so teams won’t be surprised under mixed loads.

If you want, I can provide a compact `gen_statem` skeleton that includes `:re_syncing`, `session_id` gating, and the busy-retry code wired up—ready to drop into `connection.ex`.
