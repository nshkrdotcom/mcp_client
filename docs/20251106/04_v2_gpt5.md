Awesome brief. Here’s a surgical, OTP-native refinement that prioritizes mechanical sympathy with the BEAM, clean failure semantics, and pragmatic integration. I’ve kept scope tight: mostly re-architecture and contracts—no feature creep.

---

# 1) Complete state machine

## States

* `:starting` – spawn/attach transport, set active(:once)
* `:initializing` – run MCP initialize; negotiate caps
* `:ready` – normal ops
* `:backoff` – exponential backoff, reconnect loop
* `:closing` – graceful shutdown

## Classes of events

* **ctl**: start/stop, user calls, timeouts
* **io**: frames from transport, send results, transport exits
* **proto**: JSON-RPC requests/responses/notifications
* **int**: internal (state timeouts, backoff ticks)

## Transition table (edges labeled with `event/guard ⇒ actions`)

```
:start
  ctl({:spawn_transport, opts})                       ⇒ spawn_link(transport), set_active(:once), → :initializing

:initializing
  io({:transport_up})                                 ⇒ no-op (we’re already up)
  io({:frame, msg}) / is_init_ok(msg)                 ⇒ set caps, reply “initialized”, flush pending init waiters, → :ready
  io({:frame, msg}) / is_init_err(msg)                ⇒ log(protocol), → :backoff(init_reason)
  int({:state_timeout, init_tmo, :init_timeout})      ⇒ → :backoff(:init_timeout)
  io({:transport_down, reason})                       ⇒ → :backoff({:transport_down, reason})
  ctl(stop)                                           ⇒ close(), → :closing
  io({:frame, _}) / otherwise                         ⇒ drop (unsafe during init), set_active(:once)

:ready
  io({:frame, rsp}) / is_response(rsp) & match_id     ⇒ deliver_reply(rsp), clear_timer(id), set_active(:once), stay
  io({:frame, req}) / is_server_request(req)          ⇒ handle_server_req(req) (async), set_active(:once), stay
  io({:frame, ntf}) / is_notification(ntf)            ⇒ dispatch_notification(ntf) (async), set_active(:once), stay
  proto({:server_broadcast, :"notifications/cancelled"}) ⇒ cancel_all_local(), clear_request_table(), → :initializing  (re-sync)
  ctl({:user_call, method, params, opts})             ⇒ id=next_id(), put_req(id, from, timer), send_frame(), stay
  int({:state_timeout, {:request_timeout, id}})       ⇒ cancel_upstream(id), reply(from, {:error, :timeout}), tombstone(id), stay
  io({:transport_down, reason})                       ⇒ → :backoff({:transport_down, reason})
  ctl(stop)                                           ⇒ close(), → :closing

:backoff
  int({:state_timeout, {:backoff_tick, delay}})       ⇒ attempt_reconnect(), if ok → :initializing else schedule next tick, stay
  io({:frame, _})                                     ⇒ drop (no guarantees), set_active(:once)
  io({:transport_up})                                 ⇒ begin_init(), → :initializing
  ctl({:user_call, _, _, _})                          ⇒ reply(:unavailable), stay
  ctl(stop)                                           ⇒ close(), → :closing

:closing
  io({:transport_down, _})                            ⇒ flush waiters({:error, :closing}), stop(normal)
  int({:state_timeout, :drain})                       ⇒ stop(normal)
  (any)                                               ⇒ ignore
```

### Guards & actions (idempotent)

* `is_init_ok/1`: Server replied `initialize/result` with caps; action is idempotent—repeat sets same caps.
* `cancel_upstream/1`: Fire JSON-RPC `$/cancelRequest` *best-effort*. If response already arrived, tombstone suppresses duplicate delivery.
* **Tombstones**: ETS set with TTL (see §6) for late responses; expire via periodic sweep (state timeout) to bound memory.
* **Backoff**: Exponential with jitter; capped; reset on successful init.

**Receiving a response in `:backoff`**
Drop. We’ve lost liveness guarantees; only “ready” delivers user-visible results. Late frames get tombstone-filtered on reattach anyway.

**Server-wide cancellation (`notifications/cancelled`)**
Treat as capability-/session-reset. Clear local pending table, tombstone ids (short TTL), and transition to `:initializing` to re-negotiate.

---

# 2) Supervision hierarchy

```
McpClient.ConnectionSupervisor (rest_for_one)
  ├─ McpClient.Transport (worker)           # stdio/sse/http_sse; supervised
  ├─ McpClient.TaskSupervisor (supervisor)  # for notifications/progress/jobs
  ├─ McpClient.Metrics (worker, optional)   # aggregation counters (in-proc)
  └─ McpClient.Connection (worker)          # gen_statem; started LAST
```

**Why `rest_for_one`?**

* **Transport dies** ⇒ Connection depends on it; connection must restart (re-init, rebuild timers, clear flight table). With `rest_for_one`, children *after* the failed child restart. We place **Connection last**, so a transport crash restarts *transport + connection*, but **not** the TaskSupervisor. Tasks are ephemeral anyway.
* **TaskSupervisor dies** ⇒ Only it restarts; connection keeps running; you might miss some notifications dispatch, not protocol-critical.
* **Connection dies** (bug) ⇒ It restarts *alone* (last child). Transport remains up, but the new Connection will re-bind to the existing transport PID? **We avoid this footgun**: Connection *links* to Transport PID and treats foreign transports as invalid at init; on connection crash, restart strategy will also restart transport because of the link exit—thus both come up cleanly.

**Proved failure scenarios**

* Transport EXIT → restarts transport and (since linked) triggers connection restart; pending requests fail with `:transport_down` and re-init occurs.
* Request table can’t get out of sync: it lives in the Connection (see §3), so its lifetime matches the Connection’s.

---

# 3) Request manager: removed (folded into the state machine)

**Rationale**

* One hot path, zero extra hops, fewer processes → less scheduler thrash.
* Timers use `:gen_statem` `:state_timeout` / `:event_timeout` actions—no swarm of process timers.

**Implementation notes**

* `data.requests`: ETS `:set` owned by Connection (or plain map if low volume). Row: `{id, from, deadline_mono, correlation_id}`.
* `data.tombstones`: ETS `:set` of `{id, expiry_mono}`; swept every `tombstone_sweep_ms`.
* On user call: insert row; schedule event timeout with `{request_timeout, id}` using absolute `deadline_mono` to avoid drift.
* On response: `take` row; if missing but in tombstones → drop.

---

# 4) Backpressure & CPU isolation

**Goals**: bound mailbox, bound latency, avoid long reductions in the Connection.

**Flow**

```
Port/SSE → Transport (active: once) 
  ⇒ delivers {:frame, binary} to Connection
Connection
  ⇒ hand-off JSON decode to a short-lived worker (Task.Supervisor) when payload size > decode_threshold_bytes
     - small frames: decode inline (fast path; fewer context switches)
     - large frames: decode in Task; Connection immediately set_active(:once) to continue reading
  ⇒ after decode, Task sends {:decoded, parsed} back to Connection
  ⇒ Connection handles parsed message (route request/response/notification)
  ⇒ set_active(:once) after *each* message handled (or after scheduling decode task)
```

**Bounded buffers**

* Transport only enables `:once` after Connection explicitly calls it.
* Connection tracks **in-flight decode tasks** count and caps (e.g. `max_decoders = 8`). If cap reached, it *defers* `set_active(:once)` until a decoder finishes—this is a natural backpressure valve across the whole pipeline.
* Notification dispatch runs in TaskSupervisor per handler; dispatch tasks are **not awaited**; they must be side-effect safe. Optionally cap concurrent dispatch with a semaphore (simple `:ets` counter or `:atomics`).

**Latency**

* Small frames: decode inline, O(milliseconds).
* Big frames: payload offloaded; Connection remains responsive (keeps enabling next frame unless decoder cap is hit).

---

# 5) Transport behaviour contract (refined)

Split into lifecycle (supervised), data plane, and diagnostics.

```elixir
defmodule McpClient.Transport do
  @moduledoc """
  A transport is a supervised process that frames JSON-RPC messages and
  delivers complete frames to the Connection via messages.
  """

  @typedoc "Opaque transport pid started under ConnectionSupervisor"
  @type t :: pid()

  @callback start_link(keyword()) :: GenServer.on_start()
  @callback send_frame(t(), iodata()) :: :ok | {:error, :busy | :closed | term()}
  @callback set_active(t(), :once | false) :: :ok | {:error, term()}
  @callback close(t()) :: :ok

  # Optional, for observability (not used in hot path)
  @callback get_stats(t()) :: map()
end
```

**Runtime contract**

* Transport **must** send only complete frames to the Connection process as:

  * `{:transport, :frame, binary_or_iolist}` (Connection will decide decode path)
  * `{:transport, :up}` exactly once after ready
  * `{:transport, :down, reason}` on shutdown
* `send_frame/2` is **non-blocking**; return `{:error, :busy}` if internal buffer is saturated. Connection will retry with a short backoff or fail the request (configurable). This avoids scheduler stalls.
* **Supervision**: The transport is started by the supervisor, not self-managed. The Connection holds its PID and links to it for lifecycle coupling (see §2).

**Integration tests (transport)**

1. **Framing correctness**: NDJSON multi-chunk → emits exact one frame per JSON.
2. **Active-once discipline**: No frames are delivered unless `set_active(:once)` is called.
3. **Saturation**: under synthetic slow consumer, `send_frame/2` returns `:busy`.
4. **Reconnection**: after external kill, transport restarts cleanly and re-emits `:up`.

---

# 6) Cancellation protocol (formalized)

### Data

* `requests`: `{id, from, deadline_mono, corr_id}`
* `tombstones`: `{id, expiry_mono}` (TTL = `cancel_tombstone_ms`, e.g. 30_000)

### Rules

* **Client-initiated** (timeout or caller drop):

  1. If row present → remove row, reply `{:error, :timeout | :cancelled}` to caller (if alive).
  2. Send `$/cancelRequest` (best-effort). If `send_frame/2` returns `:busy`, enqueue a single retry; if it still fails, drop (tombstone protects).
  3. Insert tombstone(id) with expiry.

* **Server-initiated** (`notifications/cancelled`):

  * Clear `requests`, reply `{:error, :cancelled_by_server}` to all, tombstone all ids, transition to `:initializing`.

* **Races**

  * Response arrives *while* sending cancel: if row already removed, match falls through; tombstone suppresses duplicate reply.
  * Late response after timeout: dropped due to tombstone.
  * Server lacks cancel support: cancel still sent (harmless); we rely on tombstones to suppress late responses.

### Timing (ASCII)

```
Caller           Connection               Server
  | call(id)        | send_frame(id)        |
  |---------------->|---------------------->|
  |                 | start timer(id)       |
  |                 |                       |
  |   timeout       | cancel_upstream(id)   |
  |<----------------|---------------------->|
  |                 | tombstone(id, TTL)    |
  |                 |                       |
  |                 |   (late response id)  |
  |                 |<----------------------|
  |                 | drop (tombstone)      |
```

Tombstones are swept every `tombstone_sweep_ms` (e.g. 10s) to bound memory.

---

# 7) Connection identity & multiplexing stance

**Scope (explicit)**: *single logical connection per server endpoint per client process*.

* No pooling: JSON-RPC/MCP isn’t typically fan-out heavy and benefits from ordered semantics per connection.
* **Identity**: expose `client_info` as provided by the user; library does not persist identity tokens across restarts (that’s application concern).
* **Subscriptions**: remain local to a connection. If applications need cross-connection coordination, they should register with a shared `Registry` at the app layer. We’ll provide a tiny helper `McpClient.PubSub` example later if needed.

Tradeoff: keeps the library simple and mechanically correct; avoids hidden distributed semantics that many OTP apps would rather control.

---

# 8) Error handling: repair matrix

| Kind         | Example                            | Connection action                             | User effect                                     |
| ------------ | ---------------------------------- | --------------------------------------------- | ----------------------------------------------- |
| `:transport` | Port closed, TCP reset             | → `:backoff`; restart transport; re-init      | In-flight calls `{:error, :transport_down}`     |
| `:protocol`  | Malformed JSON / unexpected method | Log at `warn`, drop frame, continue           | None unless tied to request; then error         |
| `:jsonrpc`   | -32601 method not found            | Deliver error to caller                       | Caller receives server error                    |
| `:state`     | Response for unknown id            | If tombstone → drop; else warn + drop         | None                                            |
| `:init`      | Handshake failure / timeout        | → `:backoff` with exponential retry           | API calls blocked with `{:error, :unavailable}` |
| `:busy`      | send_frame returns :busy           | Retry once with small delay else fail request | Caller gets `{:error, :backpressure}`           |

---

# 9) Telemetry: correlation, spans, sampling

* **Correlation ID**: `corr_id = <<system_time::64, unique_int::32>>` generated per request; included in all request events and logs.
* **Spans**: use `:telemetry.span/3` around request send/receive.
* **Sampling**:

  * `:telemetry_sample_rate` (0.0–1.0) applied to *start* event; if not sampled, do not emit stop/exception either.
* **Aggregation**: optional `McpClient.Metrics` worker maintains counters (requests/sec, bytes in/out) with `:telemetry` periodic reports (e.g., per 1s) to avoid high-cardinality storms.

Event schema (stable):

* `[:mcp_client, :request, :start|:stop|:exception]`

  * `meta`: `%{client: pid(), method: String.t(), id: non_neg_integer(), corr_id: binary()}`
  * `measurements`: `%{system_time: integer()} | %{duration: native_time()}`
* `[:mcp_client, :connection, :transition]`

  * `meta`: `%{from: atom(), to: atom(), reason: term() | nil}`
  * `measurements`: `%{}`
* `[:mcp_client, :transport, :io]`

  * `meta`: `%{dir: :in|:out, bytes: non_neg_integer()}`

---

# 10) Property tests (StreamData) you actually want

1. **Correlation 1:1 under reordering**

   * Generate a sequence of N requests with unique ids.
   * Feed responses in arbitrary permutations (including duplicates and unknown ids).
   * Property: each request receives exactly one terminal outcome; no duplicates; unknown ids do not crash the state machine.

2. **Cancellation idempotency**

   * For each request, call cancel 0–k times; interleave with late responses.
   * Property: caller receives at most one terminal outcome; connection remains in `:ready`.

3. **Timeout does not leak**

   * Generate timeouts via shrinking deadlines.
   * Property: after run, `data.requests` and timers are empty; tombstones <= N and decay after TTL.

4. **Acyclic transitions**

   * Randomly generate sequences of events (up/down, frames, init errors).
   * Property: there exists no cycle that leaves `:backoff` without a successful `:initializing → :ready` transition or an eventual stop.

5. **At-least-once notification dispatch**

   * Deliver burst of K notifications under load; allow task failures.
   * Property: every notification arrives at ≥1 registered handler (use counters); handler failures don’t terminate Connection.

---

# 11) Request lifecycle (concise flow)

```
McpClient.call_tool/4
  → Connection (call) 
      id = next_id()
      put requests[id] = {from, deadline, corr_id}
      span start
      case Transport.send_frame(...)
        :ok     → set_active(:once); set event_timeout(deadline)
        :busy   → schedule retry once else fail with {:error, :backpressure}
  ← Response frame
      if tombstoned(id) → drop
      else if take(id) → span stop; reply(from, result)
  ← Timeout event
      if take(id) → send cancel; tombstone(id); reply timeout
```

---

# 12) Updated module tree (tightened)

```
lib/mcp_client/
  application.ex
  connection.ex                 # gen_statem (owns requests & timers)
  transport/
    behaviour.ex                # contract above
    stdio.ex
    sse.ex
    http_sse.ex
  protocol/
    jsonrpc.ex                  # encode/decode helpers (pure)
    initialize.ex
    ping.ex
    logging.ex
  features/
    resources.ex
    prompts.ex
    tools.ex
    sampling.ex
    roots.ex
  middleware/
    telemetry.ex                # spans + sampling
    logging.ex
    retry.ex                    # (internal, for :busy resend only)
  errors.ex
  telemetry/schema.ex           # docs for events
  supervisors/
    connection_supervisor.ex    # rest_for_one
    task_supervisor.ex
  metrics.ex                    # optional aggregation (off by default)
```

> Note: **RequestManager** and **Subscription store** processes are removed unless you have multi-connection shared subscriptions. Per-connection, keep them as maps/ETS in `connection.ex`. If you *need* cross-connection sharing later, add a separate app-level Registry—not here.

---

# 13) Example integration patterns

### A) Phoenix LiveView (per-session client, transient)

```elixir
# live_view.ex
def mount(_params, _session, socket) do
  name = {:via, Registry, {MyApp.McpRegistry, "lv:#{socket.id}"}}
  spec = McpClient.child_spec(transport: :stdio, command: "npx", args: ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"], name: name)
  {:ok, _pid} = DynamicSupervisor.start_child(MyApp.McpDyn, spec |> Map.put(:restart, :temporary))
  :ok = McpClient.await_initialized(name, 15_000)
  {:ok, assign(socket, mcp: name)}
end

def terminate(_reason, %{assigns: %{mcp: name}}) do
  McpClient.stop(name)
end
```

Supervision:

```
MyApp.Supervisor
  ├─ Registry MyApp.McpRegistry
  └─ DynamicSupervisor MyApp.McpDyn
```

### B) Broadway (singleton client, permanent)

```elixir
children = [
  {McpClient, name: MyApp.MCP, transport: :http_sse, base_url: "...", sse_endpoint: "/sse", message_endpoint: "/message"},
  {MyPipeline, []}
]
```

Workers call `McpClient.call_tool(MyApp.MCP, "do_work", %{...}, timeout: 15_000)`. If connection backoffs, they get `{:error, :unavailable}` and you can `Broadway.Message.failed/2` with retry policy.

### C) Oban (job-per-call, transient)

```elixir
children = [
  {Oban, ...}, 
  {McpClient, name: MyApp.MCP, transport: :sse, url: "..."}  # or start inside the job with restart: :temporary
]

def perform(%Job{args: args}) do
  with :ok <- McpClient.await_initialized(MyApp.MCP, 30_000),
       {:ok, rsp} <- McpClient.call_tool(MyApp.MCP, "heavy_task", args, timeout: 60_000) do
    :ok
  else
    {:error, :unavailable} -> {:retry, backoff: 10}
    {:error, %McpClient.Error{} = e} -> {:discard, e}
  end
end
```

---

# 14) Config you actually need (tight defaults)

```elixir
config :mcp_client,
  initialize_timeout_ms: 10_000,
  request_timeout_ms: 30_000,
  backoff_min_ms: 200,
  backoff_max_ms: 15_000,
  backoff_jitter: 0.2,
  decode_threshold_bytes: 128_000,   # offload to Task at this size
  max_decoders: 8,
  cancel_tombstone_ms: 30_000,
  tombstone_sweep_ms: 10_000,
  telemetry_sample_rate: 0.1
```

---

# 15) Things I’m intentionally leaving as options (with tradeoffs)

* **Decode in Connection vs Task threshold**: Inline decoding is faster for small frames; Task offload improves tail latency for huge blobs but adds context switches. Threshold makes it tunable; defaults above are safe.
* **ETS vs Map for requests**: ETS scales better under contention; Map is simpler and faster for small N; pick Map by default, ETS behind a feature flag if you expect 1k+ concurrency.
* **Dropping frames in `:backoff`**: We drop to simplify; buffering would risk unbounded memory and stale state.

---

If you’d like, I can follow up with a ready-to-paste `connection.ex` skeleton (state definitions + handlers), or a minimal NDJSON `stdio` transport that obeys the contract.
